{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install paramiko\n",
    "sentence=[['sudesh', 'South Florida','is','Good'],['Mohit','Boy'],['Uday','man','Good',\"South Florida\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence – list of list of our corpus\n",
    "min_count=1 -the threshold value for the words. Word with frequency greater than this only are going to be included into the model.\n",
    "size=300 – the number of dimensions in which we wish to represent our word. This is the size of the word vector.\n",
    "workers=4 – used for parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027305556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohitdedhe/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#using the model\n",
    "#The new trained model can be used similar to the pre-trained ones.\n",
    "\n",
    "#printing similarity index\n",
    "print(model.similarity('Uday', 'sudesh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the downloaded model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#the model is loaded. It can be used to perform all of the tasks mentioned above.\n",
    "\n",
    "# getting word vectors of a word\n",
    "dog = model['dog']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431607246399), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411999702454)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#performing king queen magic\n",
    "print(model.most_similar(positive=['woman', 'king'], negative=['man']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#picking odd one out\n",
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76640123\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#printing similarity index\n",
    "print(model.similarity('woman', 'man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "          'Text of first document.',\n",
    "          'Text of the second document made longer.',\n",
    "          'Number three.',\n",
    "          'This is number four.',\n",
    "          'This is number five'\n",
    "]\n",
    "# learn the vocabulary and store CountVectorizer sparse matrix in X\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "# columns of X correspond to the result of this method\n",
    "vectorizer.get_feature_names() == (\n",
    "    ['document',\n",
    " 'first',\n",
    " 'five',\n",
    " 'four',\n",
    " 'is',\n",
    " 'longer',\n",
    " 'made',\n",
    " 'number',\n",
    " 'of',\n",
    " 'second',\n",
    " 'text',\n",
    " 'the',\n",
    " 'this',\n",
    " 'three'])\n",
    "# retrieving the matrix in the numpy form\n",
    "X=X.toarray()\n",
    "# transforming a new document according to learn vocabulary\n",
    "vectorizer.transform(['A new document five.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['document',\n",
       " 'first',\n",
       " 'five',\n",
       " 'four',\n",
       " 'is',\n",
       " 'longer',\n",
       " 'made',\n",
       " 'number',\n",
       " 'of',\n",
       " 'second',\n",
       " 'text',\n",
       " 'the',\n",
       " 'this',\n",
       " 'three']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45387722, 0.61805049, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.45387722, 0.        ,\n",
       "        0.45387722, 0.        , 0.        , 0.        ],\n",
       "       [0.30983299, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.42190359, 0.42190359, 0.        , 0.30983299, 0.42190359,\n",
       "        0.30983299, 0.42190359, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.50106071, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.86541213],\n",
       "       [0.        , 0.        , 0.        , 0.64364674, 0.47267431,\n",
       "        0.        , 0.        , 0.37266186, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.47267431, 0.        ],\n",
       "       [0.        , 0.        , 0.64364674, 0.        , 0.47267431,\n",
       "        0.        , 0.        , 0.37266186, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.47267431, 0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# create tf-idf object\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "# X can be obtained as X.toarray() from the previous snippet\n",
    "#X = [[3, 0, 1],\n",
    "#     [5, 0, 0],\n",
    "#     [3, 0, 0],\n",
    "#     [1, 0, 0],\n",
    "#     [3, 2, 0],\n",
    "#     [3, 0, 4]]\n",
    "# learn the vocabulary and store tf-idf sparse matrix in tfidf\n",
    "tfidf = transformer.fit_transform(X)\n",
    "# retrieving matrix in numpy form as we did it before\n",
    "tfidf.toarray()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Text', 'of', 'the', 'first', 'document.'], ['Text', 'of', 'the', 'second', 'document', 'made', 'longer.'], ['Number', 'three.'], ['This', 'is', 'number', 'four.']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "corpus = [\n",
    "          'Text of the first document.',\n",
    "          'Text of the second document made longer.',\n",
    "          'Number three.',\n",
    "          'This is number four.',\n",
    "]\n",
    "# we need to pass splitted sentences to the model\n",
    "tokenized_sentences = [sentence.split() for sentence in corpus]\n",
    "print(tokenized_sentences)\n",
    "model = word2vec.Word2Vec(tokenized_sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glove\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/c9/17c400d0c29746162bd47fc719bf3212b2b031949d41d712e9bdef11ae03/glove-1.0.2.tar.gz\n",
      "Requirement already satisfied: numpy in ./anaconda3/lib/python3.7/site-packages (from glove) (1.15.4)\n",
      "Building wheels for collected packages: glove\n",
      "  Building wheel for glove (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Users/mohitdedhe/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-install-388n_f2r/glove/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-install-388n_f2r/glove/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-wheel-4ntj6y3l --python-tag cp37\n",
      "       cwd: /private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-install-388n_f2r/glove/\n",
      "  Complete output (23 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-10.7-x86_64-3.7\n",
      "  creating build/lib.macosx-10.7-x86_64-3.7/glove\n",
      "  copying glove/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/glove\n",
      "  copying glove/glove.py -> build/lib.macosx-10.7-x86_64-3.7/glove\n",
      "  running egg_info\n",
      "  writing glove.egg-info/PKG-INFO\n",
      "  writing dependency_links to glove.egg-info/dependency_links.txt\n",
      "  writing requirements to glove.egg-info/requires.txt\n",
      "  writing top-level names to glove.egg-info/top_level.txt\n",
      "  reading manifest file 'glove.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  writing manifest file 'glove.egg-info/SOURCES.txt'\n",
      "  error: Error: setup script specifies an absolute path:\n",
      "  \n",
      "      /private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-install-388n_f2r/glove/glove/glove_inner.pyx\n",
      "  \n",
      "  setup() arguments must *always* be /-separated paths relative to the\n",
      "  setup.py directory, *never* absolute paths.\n",
      "  \n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for glove\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for glove\n",
      "Failed to build glove\n",
      "Installing collected packages: glove\n",
      "    Running setup.py install for glove ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/mohitdedhe/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-install-388n_f2r/glove/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-install-388n_f2r/glove/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-record-lkmr9mdc/install-record.txt --single-version-externally-managed --compile\n",
      "         cwd: /private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-install-388n_f2r/glove/\n",
      "    Complete output (23 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.macosx-10.7-x86_64-3.7\n",
      "    creating build/lib.macosx-10.7-x86_64-3.7/glove\n",
      "    copying glove/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/glove\n",
      "    copying glove/glove.py -> build/lib.macosx-10.7-x86_64-3.7/glove\n",
      "    running egg_info\n",
      "    writing glove.egg-info/PKG-INFO\n",
      "    writing dependency_links to glove.egg-info/dependency_links.txt\n",
      "    writing requirements to glove.egg-info/requires.txt\n",
      "    writing top-level names to glove.egg-info/top_level.txt\n",
      "    reading manifest file 'glove.egg-info/SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    writing manifest file 'glove.egg-info/SOURCES.txt'\n",
      "    error: Error: setup script specifies an absolute path:\n",
      "    \n",
      "        /private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-install-388n_f2r/glove/glove/glove_inner.pyx\n",
      "    \n",
      "    setup() arguments must *always* be /-separated paths relative to the\n",
      "    setup.py directory, *never* absolute paths.\n",
      "    \n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /Users/mohitdedhe/anaconda3/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-install-388n_f2r/glove/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-install-388n_f2r/glove/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/s7/dfw8scbj7ds___f457126zl00000gn/T/pip-record-lkmr9mdc/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'glove'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ddc6d0603bdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText8Corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mglove\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# sentences and corpus from standard library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mText8Corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'glove'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from glove import Corpus, Glove\n",
    "# sentences and corpus from standard library\n",
    "sentences = list(itertools.islice(Text8Corpus('text8'),None))\n",
    "corpus = Corpus()\n",
    "# fitting the corpus with sentences and creating Glove object\n",
    "corpus.fit(sentences, window=10)\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "# fitting to the corpus and adding standard dictionary to the object\n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "VAL_SIZE = 1000  # Size of the validation set\n",
    "NB_START_EPOCHS = 10  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "MAX_LEN = 24  # Maximum number of words in a sequence\n",
    "GLOVE_DIM = 100  # Number of dimensions of the GloVe word embeddings\n",
    "root = Path('../')\n",
    "input_path = root / 'input/'\n",
    "ouput_path = root / 'output/'\n",
    "source_path = root / 'source/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
    "    '''\n",
    "    Function to train a multi-class model. The number of epochs and \n",
    "    batch_size are set by the constants at the top of the\n",
    "    notebook. \n",
    "    \n",
    "    Parameters:\n",
    "        model : model with the chosen architecture\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_valid : validation features\n",
    "        Y_valid : validation target\n",
    "    Output:\n",
    "        model training history\n",
    "    '''\n",
    "    model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train\n",
    "                       , y_train\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=(X_valid, y_valid)\n",
    "                       , verbose=0)\n",
    "    return history\n",
    "\n",
    "\n",
    "def eval_metric(history, metric_name):\n",
    "    '''\n",
    "    Function to evaluate a trained model on a chosen metric. \n",
    "    Training and validation metric are plotted in a\n",
    "    line chart for each epoch.\n",
    "    \n",
    "    Parameters:\n",
    "        history : model training history\n",
    "        metric_name : loss or accuracy\n",
    "    Output:\n",
    "        line chart with epochs of x-axis and metric on\n",
    "        y-axis\n",
    "    '''\n",
    "    metric = history.history[metric_name]\n",
    "    val_metric = history.history['val_' + metric_name]\n",
    "\n",
    "    e = range(1, NB_START_EPOCHS + 1)\n",
    "\n",
    "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
    "    '''\n",
    "    Function to test the model on new data after training it\n",
    "    on the full training data with the optimal number of epochs.\n",
    "    \n",
    "    Parameters:\n",
    "        model : trained model\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_test : test features\n",
    "        y_test : test target\n",
    "        epochs : optimal number of epochs\n",
    "    Output:\n",
    "        test accuracy and test loss\n",
    "    '''\n",
    "    model.fit(X_train\n",
    "              , y_train\n",
    "              , epochs=epoch_stop\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def remove_stopwords(input_text):\n",
    "    '''\n",
    "    Function to remove English stopwords from a Pandas Series.\n",
    "    \n",
    "    Parameters:\n",
    "        input_text : text to clean\n",
    "    Output:\n",
    "        cleaned Pandas Series \n",
    "    '''\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) \n",
    "    \n",
    "def remove_mentions(input_text):\n",
    "    '''\n",
    "    Function to remove mentions, preceded by @, in a Pandas Series\n",
    "    \n",
    "    Parameters:\n",
    "        input_text : text to clean\n",
    "    Output:\n",
    "        cleaned Pandas Series \n",
    "    '''\n",
    "    return re.sub(r'@\\w+', '', input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Tweets.csv')\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df = df[['text', 'airline_sentiment']]\n",
    "df.text = df.text.apply(remove_stopwords).apply(remove_mentions)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Tokenizer from Keras, we convert the tweets into sequences of integers. We limit the number of words to the NB_WORDS most frequent words. Additionally, the tweets are cleaned with some filters, set to lowercase and split on spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{\"}~\\t\\n',lower=True, split=\" \")\n",
    "tk.fit_on_texts(X_train)\n",
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Equal length of sequences</h4>\n",
    "Each batch needs to provide sequences of equal length. We achieve this with the pad_sequences method. By specifying maxlen, the sequences or padded with zeros or truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Encoding the target variable</h4>\n",
    "The target classes are strings which need to be converted into numeric vectors. This is done with the LabelEncoder from Sklearn and the to_categorical method from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "y_train_oh = to_categorical(y_train_le)\n",
    "y_test_oh = to_categorical(y_test_le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Splitting off the validation set</h4>\n",
    "From the training data, we split off a validation set of 10% to use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train_oh, test_size=0.1, random_state=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Modeling\n",
    "Keras and the Embedding layer</h4>\n",
    "Keras provides a convenient way to convert each word into a multi-dimensional vector. This can be done with the Embedding layer. It will compute the word embeddings (or use pre-trained embeddings) and look up each word in a dictionary to find its vector representation. Here we will train word embeddings with 8 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mohitdedhe/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mohitdedhe/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "emb_model = models.Sequential()\n",
    "emb_model.add(layers.Embedding(NB_WORDS, 8, input_length=MAX_LEN))\n",
    "emb_model.add(layers.Flatten())\n",
    "emb_model.add(layers.Dense(3, activation='softmax'))\n",
    "emb_history = deep_model(emb_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464/1464 [==============================] - 0s 25us/step\n",
      "/n\n",
      "Test accuracy of word embeddings model: 75.48%\n"
     ]
    }
   ],
   "source": [
    "emb_results = test_model(emb_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 6)\n",
    "print('/n')\n",
    "print('Test accuracy of word embeddings model: {0:.2f}%'.format(emb_results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
